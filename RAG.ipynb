{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXVEYjvMrjTC"
      },
      "outputs": [],
      "source": [
        "# !pip install pdfplumber\n",
        "# !pip install langchain-text-splitter\n",
        "# !pip install sentence_transformers\n",
        "# !pip install faiss-cpu\n",
        "# !pip install groq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuiFupoipv97"
      },
      "source": [
        "Conversion to plain text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3wkrv3bpwjs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "\n",
        "# Ensure the folder exists\n",
        "output_folder = \"textconversion\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Define the output file path\n",
        "output_path = os.path.join(output_folder, \"output.txt\")\n",
        "\n",
        "# Extract text and write to the file\n",
        "with pdfplumber.open(\"Company-Policy-and-Procedure-June-1.18-V6.0.pdf\") as pdf, open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for page in pdf.pages:\n",
        "        t = page.extract_text()\n",
        "        if t:\n",
        "            f.write(t + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fw13CjgbpyFR"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/textconversion/output.txt\", \"r\", encoding=\"utf-8\") as document:\n",
        "    text = document.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoCmN2PzR46q",
        "outputId": "810f2ae0-a583-48bb-ceb4-d5d8549d9508"
      },
      "outputs": [],
      "source": [
        "print(len(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUTbvhfArSsI"
      },
      "source": [
        "Text Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJV5dr-KrTjk"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "texts = text_splitter.split_text(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtxvSGTqrWzs"
      },
      "source": [
        "Conversion of Chunks to Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKA5sOOJrXQR"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "vectors = model.encode(texts)\n",
        "\n",
        "vector_folder = \"vectors\"\n",
        "os.makedirs(vector_folder, exist_ok=True)\n",
        "\n",
        "output_path = os.path.join(vector_folder, \"embeddings.npy\")\n",
        "np.save(output_path, vectors)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm20RlVZZ6G0"
      },
      "source": [
        "Visualize chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s79beE1pZ8o8",
        "outputId": "dd21c5a5-fb27-4cbc-a475-f49b2a8ab166"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ VISUALIZE CHUNKS \n",
        "print(\"\\nüîç Sample Chunks and Their Lengths:\\n\")\n",
        "for i, chunk in enumerate(texts):\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "    print(chunk)\n",
        "    print(f\"Token Length: {len(chunk.split())} words\\n{'-'*40}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UN32nPTyqzq"
      },
      "source": [
        "Store Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKiYGl3mwzO1"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "dimension = vectors.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(vectors))\n",
        "\n",
        "embedding_folder = \"embeddings\"\n",
        "os.makedirs(embedding_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "index_path = os.path.join(embedding_folder, \"faiss_index.index\")\n",
        "faiss.write_index(index, index_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk2VBvvnytwR"
      },
      "source": [
        "Query Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6m9309MUxiay",
        "outputId": "aa84052f-f7bc-4ddd-ad72-b36c56a54268"
      },
      "outputs": [],
      "source": [
        "query = \"Age to acess the TL Website?\"\n",
        "query_embedding = model.encode([query])\n",
        "D, I = index.search(np.array(query_embedding), k=1)\n",
        "\n",
        "for idx in I[0]:\n",
        "    print(f\"Match: {texts[idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3EnKY_DoxO5"
      },
      "source": [
        "Importing GROQ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEIMLLGu4FNo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key=\"use your own api key lol\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXvK1yNao35G"
      },
      "source": [
        "Query Searching and LLM Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as-ptZN04KO5"
      },
      "outputs": [],
      "source": [
        "query = \"who makes the complains?\"\n",
        "query_embedding = model.encode([query])\n",
        "D, I = index.search(np.array(query_embedding), k=3)\n",
        "\n",
        "context = texts[I[0][0]]\n",
        "\n",
        "rag_prompt = f\"\"\"\n",
        "Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqzJ6-EX4PU6",
        "outputId": "e3872b3d-6b7b-46d0-d786-872de1ac8db1"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant using provided context.\"},\n",
        "        {\"role\": \"user\", \"content\": rag_prompt}\n",
        "    ],\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN0vtKaDaHZl"
      },
      "source": [
        "Query Searching with Reranking chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcyHhYRbHMIZ",
        "outputId": "22c22ad4-2698-4a5c-9e61-64535116abd6"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "query = \"summarize this\"\n",
        "\n",
        "query_embedding = embedding_model.encode([query])\n",
        "D, I = index.search(np.array(query_embedding), k=10)\n",
        "\n",
        "retrieved_chunks = [texts[i] for i in I[0]]\n",
        "\n",
        "print(\"\\nüîç Top 10 Retrieved Chunks (Pre-Rerank):\")\n",
        "for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "    print(f\"\\n[{i}] {chunk[:200]}...\")\n",
        "\n",
        "rerank_inputs = [(query, chunk) for chunk in retrieved_chunks]\n",
        "scores = reranker.predict(rerank_inputs)\n",
        "\n",
        "scored_chunks = list(zip(scores, retrieved_chunks))\n",
        "scored_chunks.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "print(\"\\nüìä Reranked Chunks (with Scores):\")\n",
        "for i, (score, chunk) in enumerate(scored_chunks, 1):\n",
        "    print(f\"\\n[{i}] Score: {score:.4f}\")\n",
        "    print(f\"{chunk[:200]}...\")\n",
        "\n",
        "top_chunks = [chunk for _, chunk in scored_chunks[:3]]\n",
        "context = \"\\n\\n\".join(top_chunks)\n",
        "\n",
        "print(\"\\nüß© Final Chunks Used in Context:\")\n",
        "for i, chunk in enumerate(top_chunks, 1):\n",
        "    print(f\"\\n[{i}] {chunk[:200]}...\")\n",
        "\n",
        "rag_prompt = f\"\"\"\n",
        "Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüß† Final RAG Prompt:\")\n",
        "print(rag_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqJe_y2OHODv",
        "outputId": "b0d16be6-2c9b-4f31-c637-762f85303a4b"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant using provided context.\"},\n",
        "        {\"role\": \"user\", \"content\": rag_prompt}\n",
        "    ],\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFLytWUKZe-f"
      },
      "source": [
        "Text splitting with meta data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVA43LNRbHly"
      },
      "source": [
        "Rag with text-splitting and meta data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kezsM8IbO7r"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = text_splitter.split_text(text)\n",
        "\n",
        "# Wrap each chunk with metadata\n",
        "texts = [\n",
        "    {\n",
        "        \"id\": i,\n",
        "        \"text\": chunk,\n",
        "        \"metadata\": {\n",
        "            \"chunk_id\": i,\n",
        "            \"start_char\": text.find(chunk),\n",
        "            \"end_char\": text.find(chunk) + len(chunk),\n",
        "            \"source\": \"your_file_name_or_path.pdf\"\n",
        "        }\n",
        "    }\n",
        "    for i, chunk in enumerate(chunks)\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CXpi6Z-bac_"
      },
      "source": [
        "Conversion of Chunks with meta data to Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHA__ac5bbHy"
      },
      "outputs": [],
      "source": [
        "# for meta data encoding we need to extarct text from dictionary it wont work directly\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "text_contents = [chunk[\"text\"] for chunk in texts]\n",
        "vectors = model.encode(text_contents)\n",
        "\n",
        "vector_folder = \"vectors\"\n",
        "os.makedirs(vector_folder, exist_ok=True)\n",
        "output_path = os.path.join(vector_folder, \"embeddings.npy\")\n",
        "np.save(output_path, vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahW49liAbmGL"
      },
      "source": [
        "For visuals of chunks with meta data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0PwWHnFbmSk",
        "outputId": "f420d992-b3df-44d7-b60a-576c88137db0"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ VISUALIZE CHUNKS \n",
        "print(\"\\nüîç Sample Chunks and Their Lengths:\\n\")\n",
        "for i, chunk in enumerate(texts):\n",
        "    print(f\"Chunk {i+1}:\")\n",
        "    print(chunk)\n",
        "    print(f\"Token Length: {len(chunk['text'].split())} words\\n{'-'*40}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPmgTz60cOY2"
      },
      "source": [
        "Store Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW4DxmqgcOlc"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "dimension = vectors.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(np.array(vectors))\n",
        "\n",
        "embedding_folder = \"embeddings\"\n",
        "os.makedirs(embedding_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "index_path = os.path.join(embedding_folder, \"faiss_index.index\")\n",
        "faiss.write_index(index, index_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4igtR5YhaQXm"
      },
      "source": [
        "Query Searching with Reranking chunks with meta data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC9ainH_ZINc",
        "outputId": "8bd0351c-6ecc-45b4-fc56-9db9da58552e"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "query = \"who makes the complains?\"\n",
        "\n",
        "query_embedding = embedding_model.encode([query])\n",
        "D, I = index.search(np.array(query_embedding), k=10)\n",
        "\n",
        "retrieved_chunks = [texts[i] for i in I[0]]\n",
        "\n",
        "print(\"\\nüîç Top 10 Retrieved Chunks (Pre-Rerank):\")\n",
        "for i, chunk in enumerate(retrieved_chunks, 1):\n",
        "    print(f\"\\n[{i}] {chunk['text'][:200]}...\")  # Access text part only\n",
        "\n",
        "rerank_inputs = [(query, chunk[\"text\"]) for chunk in retrieved_chunks]\n",
        "scores = reranker.predict(rerank_inputs)\n",
        "\n",
        "scored_chunks = list(zip(scores, retrieved_chunks))\n",
        "scored_chunks.sort(reverse=True, key=lambda x: x[0])  # Sort by score descending\n",
        "\n",
        "print(\"\\nüìä Reranked Chunks (with Scores):\")\n",
        "for i, (score, chunk) in enumerate(scored_chunks, 1):\n",
        "    print(f\"\\n[{i}] Score: {score:.4f}\")\n",
        "    print(f\"{chunk['text'][:200]}...\")\n",
        "\n",
        "top_chunks = [chunk[\"text\"] for _, chunk in scored_chunks[:3]]\n",
        "context = \"\\n\\n\".join(top_chunks)\n",
        "\n",
        "print(\"\\nüß© Final Chunks Used in Context:\")\n",
        "for i, chunk in enumerate(top_chunks, 1):\n",
        "    print(f\"\\n[{i}] {chunk[:200]}...\")\n",
        "\n",
        "rag_prompt = f\"\"\"\n",
        "Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nüß† Final RAG Prompt:\")\n",
        "print(rag_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezk-K-33dIae",
        "outputId": "01f3457d-ab71-4d8b-c39f-df6c96c1b75a"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant using provided context.\"},\n",
        "        {\"role\": \"user\", \"content\": rag_prompt}\n",
        "    ],\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
